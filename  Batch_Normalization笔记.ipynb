{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>目录<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Batch-Normalization笔记\" data-toc-modified-id=\"Batch-Normalization笔记-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Batch Normalization笔记</a></span><ul class=\"toc-item\"><li><span><a href=\"#引包\" data-toc-modified-id=\"引包-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>引包</a></span></li><li><span><a href=\"#构建模型:\" data-toc-modified-id=\"构建模型:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>构建模型:</a></span></li><li><span><a href=\"#构建训练函数\" data-toc-modified-id=\"构建训练函数-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>构建训练函数</a></span></li><li><span><a href=\"#结论\" data-toc-modified-id=\"结论-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>结论</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将会用MNIST数据集来演示这个batch normalization的使用, 以及他所带来的效果:\n",
    "\n",
    "### 引包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T08:24:56.140528Z",
     "start_time": "2018-03-03T08:24:46.037563Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T09:51:56.624641Z",
     "start_time": "2018-03-03T09:51:56.584331Z"
    }
   },
   "outputs": [],
   "source": [
    "def model1(input, is_training, keep_prob):\n",
    "    input = tf.reshape(input, shape=[-1, 28, 28, 1])\n",
    "    batch_norm_params = {\n",
    "        'decay': 0.95,\n",
    "        'updates_collections': None\n",
    "    }\n",
    "    \n",
    "    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params,\n",
    "                            activation_fn=tf.nn.crelu):\n",
    "            conv1 = slim.conv2d(input, 16, 5, scope='conv1')\n",
    "            pool1 = slim.max_pool2d(conv1, 2, scope='pool1')\n",
    "            conv2 = slim.conv2d(pool1, 32, 5, scope='conv2')\n",
    "            pool2 = slim.max_pool2d(conv2, 2, scope='pool2')\n",
    "            flatten = slim.flatten(pool2)\n",
    "            fc = slim.fully_connected(flatten, 1024, scope='fc1')\n",
    "            print(fc.get_shape())\n",
    "            drop = slim.dropout(fc, keep_prob=keep_prob)\n",
    "            logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\n",
    "            \n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T08:24:56.198451Z",
     "start_time": "2018-03-03T08:24:56.174369Z"
    }
   },
   "outputs": [],
   "source": [
    "def model2(input, is_training, keep_prob):\n",
    "    input = tf.reshape(input, shape=[-1, 28, 28, 1])\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                            normalizer_fn=None, activation_fn=tf.nn.crelu):\n",
    "        with slim.arg_scope([slim.dropout], is_training=is_training):\n",
    "            conv1 = slim.conv2d(input, 16, 5, scope='conv1')\n",
    "            pool1 = slim.max_pool2d(conv1, 2, scope='pool1')\n",
    "            conv2 = slim.conv2d(pool1, 32, 5, scope='conv2')\n",
    "            pool2 = slim.max_pool2d(conv2, 2, scope='pool2')\n",
    "            flatten = slim.flatten(pool2)\n",
    "            fc = slim.fully_connected(flatten, 1024, scope='fc1')\n",
    "            print(fc.get_shape())\n",
    "            drop = slim.dropout(fc, keep_prob=keep_prob)\n",
    "            logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\n",
    "            \n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:09:51.675949Z",
     "start_time": "2018-03-03T10:09:51.367454Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, model_path, train_log_path, test_log_path):\n",
    "    # 计算图\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        X = tf.placeholder(dtype=tf.float32, shape=[None, 28 * 28])\n",
    "        Y = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "        is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "        logit = model(X, is_training, 0.7)\n",
    "\n",
    "        loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "        accuray = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logit, 1), tf.argmax(Y, 1)), tf.float32))\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.95, staircase=True)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        update = slim.learning.create_train_op(loss, optimizer, global_step)\n",
    "        \n",
    "        mnist = input_data.read_data_sets(\"tmp\", one_hot=True)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"accuracy\", accuray)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        train_summary_writter = tf.summary.FileWriter(train_log_path, graph=tf.get_default_graph())\n",
    "        test_summary_writter = tf.summary.FileWriter(test_log_path, graph=tf.get_default_graph())\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        iter_num = 10000\n",
    "        batch_size = 1024\n",
    "\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'  # 选择cuda的设备\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)  # gpu显存使用\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(model_path)):\n",
    "                os.makedirs(os.path.dirname(model_path))\n",
    "            else:\n",
    "                try:\n",
    "                    saver.restore(sess, model_path)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for i in range(iter_num):\n",
    "                x, y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                sess.run(update, feed_dict={X:x, Y:y, is_training:True})\n",
    "\n",
    "                if i  % 100 == 0:\n",
    "                    x_test, y_test = mnist.test.next_batch(batch_size)\n",
    "\n",
    "                    print(\"train:\", sess.run(accuray, feed_dict={X: x, Y: y, is_training:False}))\n",
    "                    print(\"test:\", sess.run(accuray, feed_dict={X: x_test, Y: y_test, is_training:False}))\n",
    "\n",
    "                    saver.save(sess, model_path)\n",
    "\n",
    "                    g, summary = sess.run([global_step, merged_summary_op], feed_dict={X: x, Y: y, is_training:False})\n",
    "                    train_summary_writter.add_summary(summary, g)\n",
    "                    train_summary_writter.flush()\n",
    "\n",
    "                    g, summary = sess.run([global_step, merged_summary_op], feed_dict={X: x_test, Y: y_test, is_training:False})\n",
    "                    test_summary_writter.add_summary(summary, g)\n",
    "                    test_summary_writter.flush()\n",
    "\n",
    "        train_summary_writter.close()\n",
    "        test_summary_writter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来进行计算:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:15:52.412583Z",
     "start_time": "2018-03-03T10:09:53.476302Z"
    }
   },
   "outputs": [],
   "source": [
    "train(model1, \"model1/model\", \"model1_train_log\", \"model1_test_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T10:23:08.289864Z",
     "start_time": "2018-03-03T10:16:22.876199Z"
    }
   },
   "outputs": [],
   "source": [
    "train(model2, \"model2/model\", \"model2_train_log\", \"model2_test_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论\n",
    "\n",
    "![对比图](compare.png)\n",
    "\n",
    "我们发现, 加了batch norm的似乎收敛的更快一些, 这个我们可以从对比上可以很清楚的看到, 所以这个bn是我们一个很好的技术, 前提是你选的参数比较适合.\n",
    "\n",
    "以下是两个注意点:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys to use batch normalization in slim are:\n",
    "\n",
    "Set proper decay rate for BN layer. Because a BN layer uses EMA (exponential moving average) to approximate the population mean/variance, it takes sometime to warm up, i.e. to get the EMA close to real population mean/variance. The default decay rate is 0.999, which is kind of high for our little cute MNIST dataset and needs ~1000 steps to get a good estimation. In my code, decay is set to 0.95, then it learns the population statistics very quickly. However, a large value of decay does have it own advantage: it gathers information from more mini-batches thus is more stable.\n",
    "\n",
    "Use slim.learning.create_train_op to create train op instead of tf.train.GradientDescentOptimizer(0.1).minimize(loss) or something else!."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
